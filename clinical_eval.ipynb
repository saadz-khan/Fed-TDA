{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amax/anaconda3/envs/fl-NIH/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json,os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from fedavg.server import Server\n",
    "from fedavg.client import Client\n",
    "from fedavg.models import CNN_Model,weights_init_normal, ReTrainModel,MLP\n",
    "from utils import get_data\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_norm(train_datasets, test_dataset, cat_columns, label):\n",
    "    \n",
    "    train_data = None\n",
    "    for key in train_datasets.keys():\n",
    "        train_datasets[key]['tag'] = key\n",
    "        train_data = pd.concat([train_data, train_datasets[key]])\n",
    "    test_dataset['tag'] = key+1\n",
    "    data = pd.concat([train_data, test_dataset])\n",
    "    \n",
    "    min_max = MinMaxScaler()\n",
    "    con = []\n",
    "\n",
    "    # select continue columns\n",
    "    for c in data.columns:\n",
    "        if c not in cat_columns and c not in [label, 'tag']:\n",
    "            con.append(c)\n",
    "\n",
    "    data[con] = min_max.fit_transform(data[con])\n",
    "\n",
    "    # one-hot encode discrete columns\n",
    "    data = pd.get_dummies(data, columns=cat_columns)\n",
    "    \n",
    "    for key in train_datasets.keys():\n",
    "        c_data = data[data['tag'] == key]\n",
    "        c_data = c_data.drop(columns=['tag'])\n",
    "        train_datasets[key] = c_data\n",
    "    \n",
    "    test_dataset = data[data['tag'] == key+1]\n",
    "    test_dataset = test_dataset.drop(columns=['tag'])\n",
    "\n",
    "    return train_datasets, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(conf, train_datasets, test_dataset, device):\n",
    "    \n",
    "    ### init weight of every client node\n",
    "    client_weight = {}\n",
    "    if conf[\"is_init_avg\"]:\n",
    "        for key in train_datasets.keys():\n",
    "            client_weight[key] = 1 / len(train_datasets)\n",
    "    print(\"各节点的聚合权值为：\", client_weight)\n",
    "    \n",
    "    clients = {}\n",
    "    \n",
    "    ## init train model \n",
    "    if conf['model_name'] == \"mlp\":\n",
    "        n_input = test_dataset.shape[1] - 1\n",
    "        model = MLP(n_input, 512, conf[\"num_classes\"][conf['which_dataset']])\n",
    "    elif conf['model_name'] == 'cnn':\n",
    "        model = CNN_Model()\n",
    "    model.apply(weights_init_normal)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda(device=device)\n",
    "    \n",
    "    server = Server(conf, model, test_dataset, device)\n",
    "    print(\"Server init finish!\")\n",
    "    \n",
    "    for key in train_datasets.keys():\n",
    "        clients[key] = Client(conf, copy.deepcopy(server.global_model), train_datasets[key], device)\n",
    "    print(\"Clients init finish！\")\n",
    "\n",
    "    # save model\n",
    "    if not os.path.isdir(conf[\"model_dir\"]):\n",
    "        os.mkdir(conf[\"model_dir\"])\n",
    "    \n",
    "    return clients, server, client_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(clients, server, client_weight):\n",
    "    # fed_avg train\n",
    "    max_score = 0\n",
    "    score_list = []\n",
    "    loss_list = []\n",
    "    for e in range(conf[\"global_epochs\"]):\n",
    "        \n",
    "        clients_models = {}\n",
    "        for key in clients.keys():\n",
    "#             print('training client {}...'.format(key))\n",
    "            model_k = clients[key].local_train(server.global_model)\n",
    "            clients_models[key] = copy.deepcopy(model_k)\n",
    "\n",
    "    #         acc, loss = test_model(clients_models[key], test_dataset)\n",
    "    #         print(\"client %d,Epoch %d, global_acc: %f, global_loss: %f\\n\" % (key, e, acc, loss))\n",
    "\n",
    "\n",
    "        # fed_avg agra\n",
    "        server.model_aggregate(clients_models, client_weight)\n",
    "\n",
    "        # evaluate global model \n",
    "    #     acc, loss, auc_roc, f1 = server.model_eval()\n",
    "        acc, loss, auc_roc = server.model_eval()\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        if conf['num_classes'][conf['which_dataset']] == 2:\n",
    "            score_list.append(auc_roc)\n",
    "            print(\"Epoch %d, global_loss: %f, auc_roc: %f\" % (e, loss, auc_roc))\n",
    "            if auc_roc > max_score:\n",
    "                torch.save(server.global_model.state_dict(),\n",
    "                           os.path.join(conf[\"model_dir\"], \"global-model.pth\"))\n",
    "                for key in clients.keys():\n",
    "                    torch.save(clients[key].local_model.state_dict(),\n",
    "                               os.path.join(conf[\"model_dir\"], \"local-model{}.pth\".format(key)))\n",
    "#                 torch.save(server.global_model.state_dict(), \n",
    "#                            os.path.join(conf[\"model_dir\"], \"model-epoch{}.pth\".format(e)))\n",
    "    #             print(\"model save done !\")\n",
    "                max_score = auc_roc\n",
    "                maxe = e\n",
    "        else:\n",
    "            score_list.append(acc)\n",
    "            print(\"Epoch %d, global_loss: %f, acc: %f\" % (e, loss, acc))\n",
    "            \n",
    "            # save best model\n",
    "            if acc > max_score:\n",
    "                torch.save(server.global_model.state_dict(),\n",
    "                           os.path.join(conf[\"model_dir\"], \"globalmodel-epoch{}.pth\".format(e)))\n",
    "                for key in clients.keys():\n",
    "                    torch.save(clients[key].local_model.state_dict(),\n",
    "                               os.path.join(conf[\"model_dir\"], \"local-model{}-epoch{}.pth\".format(key, e)))\n",
    "#                 torch.save(server.global_model.state_dict(), \n",
    "#                            os.path.join(conf[\"model_dir\"], \"model-epoch{}.pth\".format(e)))\n",
    "    #             print(\"model save done !\")\n",
    "                max_score = acc\n",
    "                maxe = e\n",
    "        \n",
    "    print('max score = {0}, epoch = {1}'.format(max_score, maxe))\n",
    "    \n",
    "    return max_score, loss_list, score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_train(conf, dataset_name, b, clients_num, path, label_name, train_gpu):\n",
    "    \n",
    "    train_files_path_list = [path + \"b={}/\".format(b) + label_name + \"_{}.csv\".format(i) for i in range(clients_num)]\n",
    "    print(\"path of partition data:\\n\" + str(train_files_path_list))\n",
    "    \n",
    "    # read file\n",
    "    train_datasets = {}\n",
    "    for i in range(len(train_files_path_list)):\n",
    "        train_datasets[i] = pd.read_csv(train_files_path_list[i])\n",
    "        print(train_datasets[i][label_name].value_counts())\n",
    "    test_dataset = pd.read_csv(path + '{}_test.csv'.format(dataset_name))\n",
    "    print(\"shape of test dataset: \" + str(test_dataset.shape))\n",
    "    \n",
    "    train_datasets, test_dataset = min_max_norm(\n",
    "        train_datasets, test_dataset, \n",
    "        conf['discrete_columns'][dataset_name], \n",
    "        conf['label_column']) \n",
    "    \n",
    "    clients, server, client_weight = model_init(conf, train_datasets, test_dataset, train_gpu)\n",
    "    \n",
    "    max_score, loss_list, score_list = train_and_eval(clients, server, client_weight)\n",
    "    \n",
    "    return max_score, loss_list, score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_data(syn_data, aug_numbers,label, ratio):\n",
    "    \"\"\"\n",
    "    select augment data from synthesis data\n",
    "    \"\"\"\n",
    "  \n",
    "    aug_data = None   \n",
    "    for i in range(len(aug_numbers)):\n",
    "        \n",
    "        aug_i = syn_data[syn_data[label] == i]\n",
    "        if aug_i.shape[0] >= aug_numbers[i]:\n",
    "#             aug_data = pd.concat([aug_data, aug_i.sample(aug_numbers[i])])\n",
    "            aug_data = pd.concat([aug_data, aug_i.sample(int(ratio*len(aug_i)))])\n",
    "        else:\n",
    "            print('label {} has no enough synthetic data'.format(i))\n",
    "    \n",
    "    return aug_data\n",
    "        \n",
    "def random_aug(train_datasets, path, dataset_name, label, label_num, ratio, aug_type='same_number'):\n",
    "    \"\"\"\n",
    "    random select\n",
    "    \"\"\" \n",
    "    labels_dis = []\n",
    "    \n",
    "    for key in train_datasets.keys():\n",
    "        label_dis = []\n",
    "        for i in range(label_num):\n",
    "            label_i = len(train_datasets[key][train_datasets[key][label] == i])\n",
    "            label_dis.append(label_i)\n",
    "        labels_dis.append(label_dis)\n",
    "    labels_dis = np.array(labels_dis)\n",
    "    print(labels_dis)\n",
    "    total_dis = np.sum(labels_dis, axis=0)\n",
    "    print(total_dis)\n",
    "    aug_numbers = total_dis - labels_dis\n",
    "\n",
    "    if aug_type == 'same_number':\n",
    "        \n",
    "        for key in train_datasets.keys():\n",
    "#             syn_data = pd.read_csv('./data/clinical/syn_data/clinical_syn_{}.csv'.format(key))\n",
    "            syn_data = pd.read_csv('{0}/{1}_syn_{2}.csv'.format(path, dataset_name, key))\n",
    "            aug_data = get_random_data(syn_data, aug_numbers[key],label, ratio)\n",
    "            train_datasets[key] = pd.concat([train_datasets[key], aug_data])\n",
    "            print(train_datasets[key].shape)\n",
    "    \n",
    "    return train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_train(conf, dataset_name, b, clients_num, path, label_name, label_num, augment_path, ratio, train_gpu):\n",
    "\n",
    "    train_files_path_list = [path + \"b={}/\".format(b) + label_name + \"_{}.csv\".format(i) for i in range(clients_num)]\n",
    "    print(\"path of partition data:\\n\" + str(train_files_path_list))\n",
    "    \n",
    "    # read file\n",
    "    train_datasets = {}\n",
    "    for i in range(len(train_files_path_list)):\n",
    "        train_datasets[i] = pd.read_csv(train_files_path_list[i])\n",
    "        print(train_datasets[i][label_name].value_counts())\n",
    "    test_dataset = pd.read_csv(path + '{}_test.csv'.format(dataset_name))\n",
    "    print(\"shape of test dataset: \" + str(test_dataset.shape))\n",
    "    \n",
    "    train_datasets = random_aug(train_datasets, augment_path, dataset_name, label_name, label_num, ratio)\n",
    "    \n",
    "    train_datasets, test_dataset = min_max_norm(\n",
    "        train_datasets, test_dataset, \n",
    "        conf['discrete_columns'][dataset_name], \n",
    "        conf['label_column']) \n",
    "    \n",
    "    clients, server, client_weight = model_init(conf, train_datasets, test_dataset, train_gpu)\n",
    "    \n",
    "    max_score, loss_list, score_list = train_and_eval(clients, server, client_weight)\n",
    "    \n",
    "    return max_score, loss_list, score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "\n",
    "\t#type of data: tabular, image\n",
    "\t\"data_type\" : \"tabular\",\n",
    "\n",
    "\t#select model from mlp,simple-cnn,vgg\n",
    "\t\"model_name\" : \"mlp\",\n",
    "\n",
    "\t#fed_ccvr\n",
    "\t\"no-iid\": \"\",\n",
    "\n",
    "\t\"global_epochs\" : 100,\n",
    "\n",
    "\t\"local_epochs\" : 1,\n",
    "\n",
    "\t# Dirichlet param\n",
    "\t\"beta\" : 0.05,\n",
    "\n",
    "\t\"batch_size\" : 65,\n",
    "\n",
    "\t\"weight_decay\":1e-5,\n",
    "\n",
    "    #learning rate\n",
    "\t\"lr\" : 0.001,\n",
    "\n",
    "\t\"momentum\" : 0.9,\n",
    "\n",
    "\t\"num_parties\":5,\n",
    "\n",
    "    # if set weight of different clients even\n",
    "\t\"is_init_avg\": True,\n",
    "\n",
    "    # percentage of eval dataset in total train dataset\n",
    "\t\"split_ratio\": 0.2,\n",
    "\n",
    "    # name of the column using as label in ml mission\n",
    "\t\"label_column\": \"label\",\n",
    "\n",
    "    # path to save model\n",
    "\t\"model_dir\":\"./save_model/clinical\",\n",
    "\n",
    "    # name of saved model\n",
    "\t\"model_file\":\"model.pth\",\n",
    "    \n",
    "    # which dataset is using in current mission\n",
    "    \"which_dataset\": \"clinical\",\n",
    "    \n",
    "    \"num_classes\": {\n",
    "        \"clinical\": 2,\n",
    "        \"credit\": 2,\n",
    "        \"tb\": 2,\n",
    "        \"covtype\": 7,\n",
    "        \"intrusion\": 10,\n",
    "    },\n",
    "    \"discrete_columns\": {\n",
    "        \"adult\":[\n",
    "            'workclass',\n",
    "            'education',\n",
    "            'marital_status',\n",
    "            'occupation',\n",
    "            'relationship',\n",
    "            'race',\n",
    "            'gender',\n",
    "            'native_country'\n",
    "        ],\n",
    "        \"intrusion\":['protocol_type', 'service', 'flag'],\n",
    "        \"credit\":[],\n",
    "        \"covtype\":\n",
    "            ['Wilderness_Area4', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Soil_Type40', 'Soil_Type1',\n",
    "             'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9',\n",
    "             'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', \n",
    "             'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', \n",
    "             'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', \n",
    "             'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', \n",
    "             'Soil_Type38', 'Soil_Type39'],\n",
    "        \"clinical\":[\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "划分数据目录如下:\n",
      "['./data/clinical/b=0.05/label_0.csv', './data/clinical/b=0.05/label_1.csv', './data/clinical/b=0.05/label_2.csv', './data/clinical/b=0.05/label_3.csv', './data/clinical/b=0.05/label_4.csv']\n",
      "1    13\n",
      "Name: label, dtype: int64\n",
      "1    25\n",
      "Name: label, dtype: int64\n",
      "1    29\n",
      "Name: label, dtype: int64\n",
      "0    48\n",
      "Name: label, dtype: int64\n",
      "0    94\n",
      "Name: label, dtype: int64\n",
      "测试数据格式如下: (90, 13)\n",
      "[[ 0 13]\n",
      " [ 0 25]\n",
      " [ 0 29]\n",
      " [48  0]\n",
      " [94  0]]\n",
      "[142  67]\n",
      "(1013, 13)\n",
      "(1025, 13)\n",
      "(1029, 13)\n",
      "(1048, 13)\n",
      "(1094, 13)\n",
      "各节点的聚合权值为： {0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2}\n",
      "Server初始化完成!\n",
      "参与方初始化完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amax/swarm-learning/hty/Non-iid研究/Fed-TDA/fedavg/models.py:109: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(m.weight, 0.0, 0.01)\n",
      "/home/amax/swarm-learning/hty/Non-iid研究/Fed-TDA/fedavg/models.py:110: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  torch.nn.init.constant(m.bias, 0.0)\n",
      "/home/amax/swarm-learning/hty/Non-iid研究/Fed-TDA/fedavg/models.py:106: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(m.weight, 1.0, 0.02)\n",
      "/home/amax/swarm-learning/hty/Non-iid研究/Fed-TDA/fedavg/models.py:107: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  torch.nn.init.constant(m.bias, 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, global_loss: 0.015296, auc_roc: 0.746750\n",
      "Epoch 1, global_loss: 0.014833, auc_roc: 0.965517\n",
      "Epoch 2, global_loss: 0.013319, auc_roc: 0.960430\n",
      "Epoch 3, global_loss: 0.012910, auc_roc: 0.946297\n",
      "Epoch 4, global_loss: 0.012412, auc_roc: 0.934426\n",
      "Epoch 5, global_loss: 0.012582, auc_roc: 0.962125\n",
      "Epoch 6, global_loss: 0.012475, auc_roc: 0.967778\n",
      "Epoch 7, global_loss: 0.012505, auc_roc: 0.986433\n",
      "Epoch 8, global_loss: 0.012906, auc_roc: 0.972301\n",
      "Epoch 9, global_loss: 0.012810, auc_roc: 0.929904\n",
      "Epoch 10, global_loss: 0.012737, auc_roc: 0.949689\n",
      "Epoch 11, global_loss: 0.012829, auc_roc: 0.952516\n",
      "Epoch 12, global_loss: 0.012813, auc_roc: 0.928773\n",
      "Epoch 13, global_loss: 0.012957, auc_roc: 0.932730\n",
      "Epoch 14, global_loss: 0.012923, auc_roc: 0.964952\n",
      "Epoch 15, global_loss: 0.012777, auc_roc: 0.944036\n",
      "Epoch 16, global_loss: 0.013044, auc_roc: 0.945732\n",
      "Epoch 17, global_loss: 0.013077, auc_roc: 0.955907\n",
      "Epoch 18, global_loss: 0.013546, auc_roc: 0.955907\n",
      "Epoch 19, global_loss: 0.013424, auc_roc: 0.906162\n",
      "Epoch 20, global_loss: 0.013364, auc_roc: 0.920859\n",
      "Epoch 21, global_loss: 0.013768, auc_roc: 0.871114\n",
      "Epoch 22, global_loss: 0.013793, auc_roc: 0.867157\n",
      "Epoch 23, global_loss: 0.013534, auc_roc: 0.925947\n",
      "Epoch 24, global_loss: 0.013550, auc_roc: 0.897682\n",
      "Epoch 25, global_loss: 0.013768, auc_roc: 0.814019\n",
      "Epoch 26, global_loss: 0.013871, auc_roc: 0.830978\n",
      "Epoch 27, global_loss: 0.013610, auc_roc: 0.849067\n",
      "Epoch 28, global_loss: 0.013855, auc_roc: 0.883550\n",
      "Epoch 29, global_loss: 0.013705, auc_roc: 0.904466\n",
      "Epoch 30, global_loss: 0.013958, auc_roc: 0.870548\n",
      "Epoch 31, global_loss: 0.013831, auc_roc: 0.890899\n",
      "Epoch 32, global_loss: 0.013847, auc_roc: 0.872244\n",
      "Epoch 33, global_loss: 0.013670, auc_roc: 0.815715\n",
      "Epoch 34, global_loss: 0.013831, auc_roc: 0.776145\n",
      "Epoch 35, global_loss: 0.014008, auc_roc: 0.751837\n",
      "Epoch 36, global_loss: 0.013877, auc_roc: 0.820237\n",
      "Epoch 37, global_loss: 0.013780, auc_roc: 0.844545\n",
      "Epoch 38, global_loss: 0.013844, auc_roc: 0.849633\n",
      "Epoch 39, global_loss: 0.013822, auc_roc: 0.842849\n",
      "Epoch 40, global_loss: 0.014099, auc_roc: 0.708310\n",
      "Epoch 41, global_loss: 0.013727, auc_roc: 0.843414\n",
      "Epoch 42, global_loss: 0.013779, auc_roc: 0.815150\n",
      "Epoch 43, global_loss: 0.013836, auc_roc: 0.843414\n",
      "Epoch 44, global_loss: 0.013751, auc_roc: 0.842284\n",
      "Epoch 45, global_loss: 0.013608, auc_roc: 0.863765\n",
      "Epoch 46, global_loss: 0.013772, auc_roc: 0.864895\n",
      "Epoch 47, global_loss: 0.014035, auc_roc: 0.778971\n",
      "Epoch 48, global_loss: 0.013618, auc_roc: 0.842849\n",
      "Epoch 49, global_loss: 0.013753, auc_roc: 0.813454\n",
      "Epoch 50, global_loss: 0.013635, auc_roc: 0.811758\n",
      "Epoch 51, global_loss: 0.014180, auc_roc: 0.768796\n",
      "Epoch 52, global_loss: 0.014024, auc_roc: 0.732617\n",
      "Epoch 53, global_loss: 0.013813, auc_roc: 0.798191\n",
      "Epoch 54, global_loss: 0.013917, auc_roc: 0.771057\n",
      "Epoch 55, global_loss: 0.013776, auc_roc: 0.809497\n",
      "Epoch 56, global_loss: 0.014012, auc_roc: 0.756360\n",
      "Epoch 57, global_loss: 0.014284, auc_roc: 0.664217\n",
      "Epoch 58, global_loss: 0.014102, auc_roc: 0.704353\n",
      "Epoch 59, global_loss: 0.014102, auc_roc: 0.678915\n",
      "Epoch 60, global_loss: 0.014037, auc_roc: 0.672696\n",
      "Epoch 61, global_loss: 0.014137, auc_roc: 0.732052\n",
      "Epoch 62, global_loss: 0.014064, auc_roc: 0.697004\n",
      "Epoch 63, global_loss: 0.014304, auc_roc: 0.636518\n",
      "Epoch 64, global_loss: 0.014119, auc_roc: 0.693047\n",
      "Epoch 65, global_loss: 0.014248, auc_roc: 0.701526\n",
      "Epoch 66, global_loss: 0.014239, auc_roc: 0.625212\n",
      "Epoch 67, global_loss: 0.014177, auc_roc: 0.693047\n",
      "Epoch 68, global_loss: 0.014273, auc_roc: 0.659695\n",
      "Epoch 69, global_loss: 0.014539, auc_roc: 0.594121\n",
      "Epoch 70, global_loss: 0.014231, auc_roc: 0.634822\n",
      "Epoch 71, global_loss: 0.014346, auc_roc: 0.592990\n",
      "Epoch 72, global_loss: 0.014186, auc_roc: 0.599209\n",
      "Epoch 73, global_loss: 0.014139, auc_roc: 0.641605\n",
      "Epoch 74, global_loss: 0.013921, auc_roc: 0.699265\n",
      "Epoch 75, global_loss: 0.014251, auc_roc: 0.634822\n",
      "Epoch 76, global_loss: 0.014530, auc_roc: 0.551724\n",
      "Epoch 77, global_loss: 0.014253, auc_roc: 0.626343\n",
      "Epoch 78, global_loss: 0.014379, auc_roc: 0.648389\n",
      "Epoch 79, global_loss: 0.014330, auc_roc: 0.641605\n",
      "Epoch 80, global_loss: 0.014156, auc_roc: 0.651781\n",
      "Epoch 81, global_loss: 0.014162, auc_roc: 0.690786\n",
      "Epoch 82, global_loss: 0.014316, auc_roc: 0.642736\n",
      "Epoch 83, global_loss: 0.014131, auc_roc: 0.680045\n",
      "Epoch 84, global_loss: 0.013532, auc_roc: 0.803844\n",
      "Epoch 85, global_loss: 0.014025, auc_roc: 0.693612\n",
      "Epoch 86, global_loss: 0.014097, auc_roc: 0.737140\n",
      "Epoch 87, global_loss: 0.014401, auc_roc: 0.631995\n",
      "Epoch 88, global_loss: 0.014230, auc_roc: 0.670435\n",
      "Epoch 89, global_loss: 0.014470, auc_roc: 0.577728\n",
      "Epoch 90, global_loss: 0.014660, auc_roc: 0.533635\n",
      "Epoch 91, global_loss: 0.014266, auc_roc: 0.561334\n",
      "Epoch 92, global_loss: 0.014315, auc_roc: 0.625212\n",
      "Epoch 93, global_loss: 0.014185, auc_roc: 0.686829\n",
      "Epoch 94, global_loss: 0.014068, auc_roc: 0.666478\n",
      "Epoch 95, global_loss: 0.014053, auc_roc: 0.704918\n",
      "Epoch 96, global_loss: 0.014272, auc_roc: 0.663652\n",
      "Epoch 97, global_loss: 0.014237, auc_roc: 0.662521\n",
      "Epoch 98, global_loss: 0.014192, auc_roc: 0.736009\n",
      "Epoch 99, global_loss: 0.014239, auc_roc: 0.686829\n",
      "max score = 0.9864330130016958, epoch = 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9864330130016958,\n",
       " [0.015296440654330784,\n",
       "  0.014832589361402724,\n",
       "  0.013318781057993572,\n",
       "  0.012910016377766927,\n",
       "  0.012411559952629938,\n",
       "  0.012582037183973525,\n",
       "  0.012474881278143989,\n",
       "  0.01250477499432034,\n",
       "  0.01290593147277832,\n",
       "  0.012810476620992025,\n",
       "  0.012736855612860785,\n",
       "  0.012828654713100858,\n",
       "  0.01281255880991618,\n",
       "  0.012957139809926351,\n",
       "  0.012922766473558214,\n",
       "  0.012777119212680392,\n",
       "  0.013044053978390164,\n",
       "  0.013076702753702799,\n",
       "  0.013545862833658854,\n",
       "  0.01342389186223348,\n",
       "  0.013364262051052518,\n",
       "  0.013767980204688179,\n",
       "  0.01379294925265842,\n",
       "  0.013533686267005072,\n",
       "  0.013549958335028754,\n",
       "  0.013767984178331163,\n",
       "  0.013870965109931098,\n",
       "  0.013610484864976671,\n",
       "  0.013854961925082737,\n",
       "  0.013704878754085964,\n",
       "  0.013958328300052218,\n",
       "  0.013830855157640246,\n",
       "  0.013847292794121637,\n",
       "  0.013670367664761013,\n",
       "  0.013830751842922635,\n",
       "  0.014008408122592502,\n",
       "  0.013876684506734212,\n",
       "  0.013780172665913899,\n",
       "  0.01384432315826416,\n",
       "  0.013822139634026422,\n",
       "  0.01409913698832194,\n",
       "  0.013726960288153754,\n",
       "  0.013778895801968045,\n",
       "  0.01383561028374566,\n",
       "  0.013750571674770779,\n",
       "  0.013607842392391628,\n",
       "  0.01377155515882704,\n",
       "  0.014035132196214464,\n",
       "  0.013617969883812798,\n",
       "  0.013753283023834228,\n",
       "  0.013635314835442437,\n",
       "  0.014179529084099664,\n",
       "  0.014023817910088433,\n",
       "  0.013813359207577175,\n",
       "  0.013917254077063667,\n",
       "  0.013776059945424397,\n",
       "  0.014012463887532552,\n",
       "  0.014284314049614801,\n",
       "  0.014101601971520317,\n",
       "  0.014102009932200114,\n",
       "  0.014037209086947972,\n",
       "  0.014136854807535808,\n",
       "  0.014063644409179687,\n",
       "  0.01430373986562093,\n",
       "  0.014119124412536621,\n",
       "  0.014247696929507786,\n",
       "  0.014238801267411973,\n",
       "  0.014177001847161187,\n",
       "  0.014273133542802598,\n",
       "  0.014539358350965712,\n",
       "  0.014231186442905002,\n",
       "  0.014346300231085882,\n",
       "  0.014185505443149143,\n",
       "  0.014139249589708117,\n",
       "  0.013921230369144015,\n",
       "  0.014250686433580186,\n",
       "  0.014529811011420356,\n",
       "  0.014253324932522245,\n",
       "  0.01437924967871772,\n",
       "  0.01433019373151991,\n",
       "  0.014155581262376574,\n",
       "  0.014162121878729925,\n",
       "  0.01431618266635471,\n",
       "  0.01413148906495836,\n",
       "  0.01353160540262858,\n",
       "  0.014024727874332003,\n",
       "  0.014097306463453505,\n",
       "  0.014401208029852973,\n",
       "  0.014230190383063421,\n",
       "  0.014469766616821289,\n",
       "  0.014660016695658366,\n",
       "  0.014266325367821587,\n",
       "  0.014314929644266764,\n",
       "  0.01418476899464925,\n",
       "  0.014068199528588189,\n",
       "  0.01405313942167494,\n",
       "  0.014272018273671468,\n",
       "  0.014237387975056966,\n",
       "  0.0141923983891805,\n",
       "  0.014238550927903918],\n",
       " [0.7467495760316564,\n",
       "  0.9655172413793103,\n",
       "  0.9604296212549464,\n",
       "  0.9462973431317128,\n",
       "  0.9344262295081968,\n",
       "  0.9621254946297343,\n",
       "  0.9677784058790277,\n",
       "  0.9864330130016958,\n",
       "  0.9723007348784625,\n",
       "  0.929903900508762,\n",
       "  0.9496890898812889,\n",
       "  0.9525155455059355,\n",
       "  0.9287733182589034,\n",
       "  0.9327303561334087,\n",
       "  0.964951950254381,\n",
       "  0.9440361786319954,\n",
       "  0.9457320520067835,\n",
       "  0.9559072922555116,\n",
       "  0.9559072922555116,\n",
       "  0.9061616732617297,\n",
       "  0.9208592425098926,\n",
       "  0.8711136235161108,\n",
       "  0.8671565856416055,\n",
       "  0.9259468626342566,\n",
       "  0.8976823063877897,\n",
       "  0.8140192198982477,\n",
       "  0.8309779536461277,\n",
       "  0.8490672696438666,\n",
       "  0.8835500282645562,\n",
       "  0.9044657998869419,\n",
       "  0.8705483323911815,\n",
       "  0.8908988128886377,\n",
       "  0.8722442057659695,\n",
       "  0.8157150932730355,\n",
       "  0.776144714527982,\n",
       "  0.7518371961560204,\n",
       "  0.8202374222724703,\n",
       "  0.844544940644432,\n",
       "  0.8496325607687959,\n",
       "  0.8428490672696438,\n",
       "  0.7083097795364612,\n",
       "  0.8434143583945732,\n",
       "  0.8151498021481063,\n",
       "  0.8434143583945732,\n",
       "  0.8422837761447145,\n",
       "  0.8637648388920294,\n",
       "  0.8648954211418881,\n",
       "  0.7789711701526286,\n",
       "  0.8428490672696438,\n",
       "  0.8134539287733182,\n",
       "  0.8117580553985303,\n",
       "  0.7687959299039004,\n",
       "  0.7326172979084228,\n",
       "  0.7981910684002261,\n",
       "  0.7710570944036179,\n",
       "  0.8094968908988129,\n",
       "  0.7563595251554551,\n",
       "  0.6642170717919729,\n",
       "  0.7043527416619559,\n",
       "  0.6789146410401358,\n",
       "  0.672696438665913,\n",
       "  0.7320520067834936,\n",
       "  0.6970039570378747,\n",
       "  0.6365178066704352,\n",
       "  0.6930469191633692,\n",
       "  0.7015262860373093,\n",
       "  0.6252119841718485,\n",
       "  0.6930469191633691,\n",
       "  0.6596947427925384,\n",
       "  0.5941209723007348,\n",
       "  0.6348219332956473,\n",
       "  0.5929903900508762,\n",
       "  0.5992085924250989,\n",
       "  0.6416054267947993,\n",
       "  0.6992651215375918,\n",
       "  0.6348219332956473,\n",
       "  0.5517241379310345,\n",
       "  0.6263425664217073,\n",
       "  0.6483889202939513,\n",
       "  0.6416054267947991,\n",
       "  0.6517806670435273,\n",
       "  0.6907857546636518,\n",
       "  0.6427360090446579,\n",
       "  0.6800452232899944,\n",
       "  0.8038439796495195,\n",
       "  0.6936122102882984,\n",
       "  0.7371396269078576,\n",
       "  0.6319954776710005,\n",
       "  0.6704352741661955,\n",
       "  0.5777275296777841,\n",
       "  0.5336348219332956,\n",
       "  0.5613340870548332,\n",
       "  0.6252119841718484,\n",
       "  0.6868287167891465,\n",
       "  0.6664782362916902,\n",
       "  0.7049180327868854,\n",
       "  0.6636517806670436,\n",
       "  0.6625211984171848,\n",
       "  0.7360090446579989,\n",
       "  0.6868287167891465])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment_train(conf, \n",
    "              \"clinical\",\n",
    "              0.05,\n",
    "              5,\n",
    "              \"./data/clinical/\",\n",
    "              \"label\",\n",
    "              2,\n",
    "              \"./data/clinical/syn\",\n",
    "              ratio=1,\n",
    "              train_gpu=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-NIH",
   "language": "python",
   "name": "fl-nih"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
